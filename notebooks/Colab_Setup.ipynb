{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETTm1 Time Series Forecasting\n",
    "\n",
    "This notebook runs and compares four forecasting models on the ETTm1 dataset:\n",
    "1. **Seasonal Naive** - Baseline using seasonal patterns\n",
    "2. **DLinear** - Decomposition + Linear layers\n",
    "3. **LSTM** - Recurrent neural network baseline\n",
    "4. **Mamba** - State space model for sequence modeling\n",
    "\n",
    "## Setup\n",
    "1. Runtime -> Change runtime type -> **GPU** (T4 recommended)\n",
    "2. Run all cells in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected! Enable GPU: Runtime -> Change runtime type -> GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (skip if already cloned)\n",
    "import os\n",
    "if not os.path.exists('Mamba-ETTm1-Forecasting'):\n",
    "    !git clone https://github.com/steinpleiter/Mamba-ETTm1-Forecasting.git\n",
    "    %cd Mamba-ETTm1-Forecasting\n",
    "else:\n",
    "    %cd Mamba-ETTm1-Forecasting\n",
    "    !git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q pandas numpy scikit-learn matplotlib seaborn pyyaml tqdm\n",
    "\n",
    "# Install Mamba dependencies (requires specific PyTorch version)\n",
    "!pip install -q torch==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q packaging ninja\n",
    "!pip install -q causal-conv1d==1.4.0\n",
    "!pip install -q mamba-ssm==2.2.2\n",
    "\n",
    "# Ensure numpy compatibility\n",
    "!pip install -q \"numpy<2.0\"\n",
    "\n",
    "print(\"\\nDependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations\n",
    "import numpy as np\n",
    "import torch\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Quick Mamba test\n",
    "m = Mamba(d_model=64).cuda()\n",
    "x = torch.randn(1, 10, 64).cuda()\n",
    "_ = m(x)\n",
    "print(\"Mamba: OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Download and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ETTm1 dataset\n",
    "import urllib.request\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/zhouhaoyi/ETDataset/main/ETT-small/ETTm1.csv'\n",
    "urllib.request.urlretrieve(url, 'data/raw/ETTm1.csv')\n",
    "print(\"Dataset downloaded: data/raw/ETTm1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "!python scripts/preprocess_data.py --config configs/base_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Training & Evaluation\n",
    "\n",
    "### 3.1 Seasonal Naive Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Seasonal Naive baseline\n",
    "!python scripts/evaluate_baseline.py \\\n",
    "    --model seasonal_naive \\\n",
    "    --device cuda \\\n",
    "    --save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DLinear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DLinear (channel-independent mode)\n",
    "!python scripts/train_dlinear.py \\\n",
    "    --device cuda \\\n",
    "    --individual \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 64 \\\n",
    "    --config configs/base_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "!python scripts/train_lstm.py \\\n",
    "    --device cuda \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 64 \\\n",
    "    --hidden_size 128 \\\n",
    "    --num_layers 2 \\\n",
    "    --config configs/base_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Mamba Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Mamba\n",
    "!python scripts/train_mamba.py \\\n",
    "    --config configs/base_config.yaml \\\n",
    "    --device cuda \\\n",
    "    --epochs 50 \\\n",
    "    --batch_size 32 \\\n",
    "    --d_model 128 \\\n",
    "    --n_layers 4 \\\n",
    "    --patch_len 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all trained models\n",
    "!python scripts/compare_models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparison plots\n",
    "from IPython.display import Image, display\n",
    "from pathlib import Path\n",
    "\n",
    "fig_path = Path('results/figures')\n",
    "\n",
    "if (fig_path / 'model_comparison.png').exists():\n",
    "    display(Image(filename=str(fig_path / 'model_comparison.png')))\n",
    "\n",
    "if (fig_path / 'final_performance.png').exists():\n",
    "    display(Image(filename=str(fig_path / 'final_performance.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def load_results(filename):\n",
    "    path = Path('results') / filename\n",
    "    if path.exists():\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    return None\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "models = {\n",
    "    'Seasonal Naive': 'seasonal_naive_baseline_results.pkl',\n",
    "    'DLinear': 'dlinear_ETTm1_training_results.pkl',\n",
    "    'LSTM': 'lstm_ETTm1_training_results.pkl',\n",
    "    'Mamba': 'mamba_ETTm1_training_results.pkl'\n",
    "}\n",
    "\n",
    "for name, filename in models.items():\n",
    "    data = load_results(filename)\n",
    "    if data:\n",
    "        metrics = data.get('test_metrics', data.get('val_metrics', {}))\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'MAE': metrics.get('mae', float('nan')),\n",
    "            'RMSE': metrics.get('rmse', float('nan')),\n",
    "            'MASE': metrics.get('mase', float('nan')),\n",
    "            'Parameters': data.get('n_parameters', 0)\n",
    "        })\n",
    "\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values('MAE')\n",
    "    print(\"\\nModel Performance (sorted by MAE):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Calculate improvement over baseline\n",
    "    baseline_mae = df[df['Model'] == 'Seasonal Naive']['MAE'].values[0]\n",
    "    df['Improvement'] = ((baseline_mae - df['MAE']) / baseline_mae * 100).round(2)\n",
    "    df['Improvement'] = df['Improvement'].apply(lambda x: f\"+{x:.1f}%\" if x > 0 else f\"{x:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\\nImprovement over Seasonal Naive:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(df[['Model', 'MAE', 'Improvement']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No results found. Run the training cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package results for download\n",
    "!zip -r results.zip results/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('results.zip')\n",
    "print(\"\\nDownload started! Extract results.zip on your local machine.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 7. Qualitative Forecast Plots (Actual vs. Predicted)\n\nGenerate sample forecast visualizations comparing True OT vs Predicted OT for LSTM (best model) and Mamba.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate Actual vs. Predicted Forecast Plots for LSTM and Mamba\nimport os\nimport sys\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport yaml\nfrom pathlib import Path\n\n# Ensure we're in the repo directory (Colab-compatible)\nif os.path.exists('/content/Mamba-ETTm1-Forecasting'):\n    os.chdir('/content/Mamba-ETTm1-Forecasting')\n    \n# Add src to path\nif '.' not in sys.path:\n    sys.path.insert(0, '.')\n\nfrom src.data.dataset import ETTDataset\nfrom src.models.lstm_forecaster import LSTMForecaster\nfrom src.models.mamba_forecaster import MambaForecaster\n\n# Load config\nwith open('configs/base_config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\ndata_config = config['data']\ncontext_length = data_config['context_length']\nforecast_horizon = data_config['forecast_horizon']\nuse_calendar = data_config.get('use_calendar_features', True)\nenc_in = 7 + (8 if use_calendar else 0)\n\n# Load test data\ntest_path = Path('data/processed') / f\"ETTm1_test_L{context_length}_H{forecast_horizon}.pkl\"\nwith open(test_path, 'rb') as f:\n    test_data = pickle.load(f)\n\n# Load normalization stats from train data (where they are stored)\ntrain_path = Path('data/processed') / f\"ETTm1_train_L{context_length}_H{forecast_horizon}.pkl\"\nwith open(train_path, 'rb') as f:\n    train_data_file = pickle.load(f)\nnorm_stats = train_data_file['norm_stats']\n\not_mean = norm_stats['OT']['mean']\not_std = norm_stats['OT']['std']\nprint(f\"OT normalization - mean: {ot_mean:.4f}, std: {ot_std:.4f}\")\n\n# Create test dataset\ntest_dataset = ETTDataset(\n    test_data['input'],\n    test_data['target'],\n    test_data['input_calendar'],\n    test_data['future_calendar'],\n    use_calendar=use_calendar\n)\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load LSTM model\nlstm_checkpoint = torch.load('results/checkpoints/lstm_ETTm1_best.pt', map_location=device)\nlstm_config = lstm_checkpoint['config']\n\nlstm_model = LSTMForecaster(\n    seq_len=context_length,\n    pred_len=forecast_horizon,\n    enc_in=enc_in,\n    hidden_size=lstm_config.get('hidden_size', 128),\n    num_layers=lstm_config.get('num_layers', 2),\n    dropout=lstm_config.get('dropout', 0.1)\n)\nlstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])\nlstm_model = lstm_model.to(device)\nlstm_model.eval()\nprint(f\"Loaded LSTM (Val MAE: {lstm_checkpoint.get('val_mae', 'N/A'):.4f})\")\n\n# Load Mamba model\nmamba_checkpoint = torch.load('results/checkpoints/mamba_ETTm1_best.pt', map_location=device)\nmamba_config = mamba_checkpoint['config']\n\nmamba_model = MambaForecaster(\n    seq_len=context_length,\n    pred_len=forecast_horizon,\n    enc_in=enc_in,\n    d_model=mamba_config.get('d_model', 128),\n    n_layers=mamba_config.get('n_layers', 4),\n    patch_len=mamba_config.get('patch_len', 16),\n    d_state=mamba_config.get('d_state', 16),\n    dropout=mamba_config.get('dropout', 0.1),\n    compress_dim=mamba_config.get('compress_dim', 64)\n)\nmamba_model.load_state_dict(mamba_checkpoint['model_state_dict'])\nmamba_model = mamba_model.to(device)\nmamba_model.eval()\nprint(f\"Loaded Mamba (Val MAE: {mamba_checkpoint.get('val_mae', 'N/A'):.4f})\")\n\n# Select sample indices from test set (beginning, middle, end)\nn_samples = len(test_dataset)\nsample_indices = [0, n_samples // 2, n_samples - 1]\n\n# Generate predictions\nfig, axes = plt.subplots(len(sample_indices), 2, figsize=(14, 4 * len(sample_indices)))\nfig.suptitle('Forecast Comparison: Actual vs. Predicted (OT Target Variable)', fontsize=14, fontweight='bold')\n\nfor row, idx in enumerate(sample_indices):\n    sample = test_dataset[idx]\n    inputs = sample['input'].unsqueeze(0).to(device)\n    true_target = sample['target'].cpu().numpy()  # Shape: (96, 7)\n    \n    # Get OT column (last column, index 6)\n    true_ot = true_target[:, 6]\n    \n    # Denormalize true OT\n    true_ot_denorm = true_ot * ot_std + ot_mean\n    \n    with torch.no_grad():\n        # LSTM prediction\n        lstm_pred = lstm_model(inputs).cpu().numpy()[0]  # Shape: (96, 7)\n        lstm_ot = lstm_pred[:, 6]\n        lstm_ot_denorm = lstm_ot * ot_std + ot_mean\n        \n        # Mamba prediction\n        mamba_pred = mamba_model(inputs).cpu().numpy()[0]  # Shape: (96, 7)\n        mamba_ot = mamba_pred[:, 6]\n        mamba_ot_denorm = mamba_ot * ot_std + ot_mean\n    \n    time_steps = np.arange(forecast_horizon)\n    \n    # LSTM plot\n    ax_lstm = axes[row, 0]\n    ax_lstm.plot(time_steps, true_ot_denorm, 'b-', linewidth=2, label='True OT')\n    ax_lstm.plot(time_steps, lstm_ot_denorm, 'orange', linewidth=2, linestyle='--', label='Predicted OT')\n    ax_lstm.fill_between(time_steps, true_ot_denorm, lstm_ot_denorm, alpha=0.3, color='gray')\n    ax_lstm.set_xlabel('Time Steps (15-min intervals)')\n    ax_lstm.set_ylabel('OT (Oil Temperature)')\n    ax_lstm.set_title(f'LSTM - Test Sample {idx + 1}')\n    ax_lstm.legend(loc='upper right')\n    ax_lstm.grid(True, alpha=0.3)\n    \n    # Calculate MAE for this sample\n    lstm_mae = np.mean(np.abs(true_ot_denorm - lstm_ot_denorm))\n    ax_lstm.text(0.02, 0.98, f'MAE: {lstm_mae:.3f}', transform=ax_lstm.transAxes, \n                 verticalalignment='top', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    # Mamba plot\n    ax_mamba = axes[row, 1]\n    ax_mamba.plot(time_steps, true_ot_denorm, 'b-', linewidth=2, label='True OT')\n    ax_mamba.plot(time_steps, mamba_ot_denorm, 'orange', linewidth=2, linestyle='--', label='Predicted OT')\n    ax_mamba.fill_between(time_steps, true_ot_denorm, mamba_ot_denorm, alpha=0.3, color='gray')\n    ax_mamba.set_xlabel('Time Steps (15-min intervals)')\n    ax_mamba.set_ylabel('OT (Oil Temperature)')\n    ax_mamba.set_title(f'Mamba - Test Sample {idx + 1}')\n    ax_mamba.legend(loc='upper right')\n    ax_mamba.grid(True, alpha=0.3)\n    \n    # Calculate MAE for this sample\n    mamba_mae = np.mean(np.abs(true_ot_denorm - mamba_ot_denorm))\n    ax_mamba.text(0.02, 0.98, f'MAE: {mamba_mae:.3f}', transform=ax_mamba.transAxes,\n                  verticalalignment='top', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.93)\n\n# Save the figure\nfig_path = Path('results/figures')\nfig_path.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_path / 'forecast_comparison_actual_vs_predicted.png', dpi=150, bbox_inches='tight')\nprint(f\"\\nSaved forecast comparison plot to: {fig_path / 'forecast_comparison_actual_vs_predicted.png'}\")\nplt.show()\n\nprint(\"\\nForecast visualization complete!\")\nprint(\"Blue line = True OT | Orange dashed = Predicted OT\")\nprint(\"Gray shaded area shows the prediction error\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare forecasts on Train vs Val vs Test to show distribution shift effect\n# This demonstrates the model works well on in-distribution data (train/val)\n# but struggles with the test set due to non-stationarity in the time series\n\nimport os\nimport sys\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport yaml\nfrom pathlib import Path\n\n# Ensure we're in the repo directory (Colab-compatible)\nif os.path.exists('/content/Mamba-ETTm1-Forecasting'):\n    os.chdir('/content/Mamba-ETTm1-Forecasting')\n    \nif '.' not in sys.path:\n    sys.path.insert(0, '.')\n\nfrom src.data.dataset import ETTDataset\nfrom src.models.lstm_forecaster import LSTMForecaster\nfrom src.models.mamba_forecaster import MambaForecaster\n\n# Load config\nwith open('configs/base_config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\ndata_config = config['data']\ncontext_length = data_config['context_length']\nforecast_horizon = data_config['forecast_horizon']\nuse_calendar = data_config.get('use_calendar_features', True)\nenc_in = 7 + (8 if use_calendar else 0)\n\n# Load all datasets\ntrain_path = Path('data/processed') / f\"ETTm1_train_L{context_length}_H{forecast_horizon}.pkl\"\nval_path = Path('data/processed') / f\"ETTm1_val_L{context_length}_H{forecast_horizon}.pkl\"\ntest_path = Path('data/processed') / f\"ETTm1_test_L{context_length}_H{forecast_horizon}.pkl\"\n\nwith open(train_path, 'rb') as f:\n    train_data = pickle.load(f)\nwith open(val_path, 'rb') as f:\n    val_data = pickle.load(f)\nwith open(test_path, 'rb') as f:\n    test_data = pickle.load(f)\n\nnorm_stats = train_data['norm_stats']\not_mean = norm_stats['OT']['mean']\not_std = norm_stats['OT']['std']\n\n# Print distribution statistics\nprint(\"=\" * 60)\nprint(\"Dataset Distribution Statistics (normalized OT targets)\")\nprint(\"=\" * 60)\nprint(f\"Train - Mean: {train_data['target'].mean():.4f}, Std: {train_data['target'].std():.4f}\")\nprint(f\"Val   - Mean: {val_data['target'].mean():.4f}, Std: {val_data['target'].std():.4f}\")\nprint(f\"Test  - Mean: {test_data['target'].mean():.4f}, Std: {test_data['target'].std():.4f}\")\nprint()\nprint(\"Note: Test set has significantly different mean (-1.10 vs ~0)\")\nprint(\"This indicates distribution shift / non-stationarity in the data\")\nprint(\"=\" * 60)\n\n# Create datasets\ntrain_dataset = ETTDataset(train_data['input'], train_data['target'], \n                           train_data['input_calendar'], train_data['future_calendar'], use_calendar=use_calendar)\nval_dataset = ETTDataset(val_data['input'], val_data['target'],\n                         val_data['input_calendar'], val_data['future_calendar'], use_calendar=use_calendar)\ntest_dataset = ETTDataset(test_data['input'], test_data['target'],\n                          test_data['input_calendar'], test_data['future_calendar'], use_calendar=use_calendar)\n\n# Device and load LSTM model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nlstm_checkpoint = torch.load('results/checkpoints/lstm_ETTm1_best.pt', map_location=device, weights_only=False)\nlstm_config = lstm_checkpoint['config']\nlstm_model = LSTMForecaster(\n    seq_len=context_length, pred_len=forecast_horizon, enc_in=enc_in,\n    hidden_size=lstm_config.get('hidden_size', 128),\n    num_layers=lstm_config.get('num_layers', 2),\n    dropout=lstm_config.get('dropout', 0.1)\n)\nlstm_model.load_state_dict(lstm_checkpoint['model_state_dict'])\nlstm_model = lstm_model.to(device)\nlstm_model.eval()\n\n# Create comparison plot: Train vs Val vs Test\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\nfig.suptitle('LSTM Forecast Quality: Train vs Validation vs Test', fontsize=14, fontweight='bold')\n\ndatasets = [('Train', train_dataset), ('Validation', val_dataset), ('Test', test_dataset)]\n\nfor ax, (name, dataset) in zip(axes, datasets):\n    # Pick a sample from the middle of each set\n    idx = len(dataset) // 2\n    sample = dataset[idx]\n    inputs = sample['input'].unsqueeze(0).to(device)\n    true_ot = sample['target'].cpu().numpy()\n    \n    # Denormalize\n    true_ot_denorm = true_ot * ot_std + ot_mean\n    \n    with torch.no_grad():\n        pred_ot = lstm_model(inputs).cpu().numpy()[0]\n        pred_ot_denorm = pred_ot * ot_std + ot_mean\n    \n    time_steps = np.arange(forecast_horizon)\n    \n    ax.plot(time_steps, true_ot_denorm, 'b-', linewidth=2, label='True OT')\n    ax.plot(time_steps, pred_ot_denorm, 'orange', linewidth=2, linestyle='--', label='Predicted OT')\n    ax.fill_between(time_steps, true_ot_denorm, pred_ot_denorm, alpha=0.3, color='gray')\n    ax.set_xlabel('Time Steps (15-min intervals)')\n    ax.set_ylabel('OT (Oil Temperature)')\n    ax.set_title(f'{name} Set')\n    ax.legend(loc='upper right')\n    ax.grid(True, alpha=0.3)\n    \n    mae = np.mean(np.abs(true_ot_denorm - pred_ot_denorm))\n    ax.text(0.02, 0.98, f'MAE: {mae:.3f}', transform=ax.transAxes,\n            verticalalignment='top', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\nplt.subplots_adjust(top=0.88)\n\nfig_path = Path('results/figures')\nfig_path.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_path / 'train_val_test_comparison.png', dpi=150, bbox_inches='tight')\nprint(f\"\\nSaved comparison plot to: {fig_path / 'train_val_test_comparison.png'}\")\nplt.show()\n\nprint(\"\\nKey Observation:\")\nprint(\"- Train/Val: Predictions closely track true values (model learned correctly)\")\nprint(\"- Test: Systematic offset due to distribution shift in the ETTm1 dataset\")\nprint(\"- This is a known challenge in time series forecasting with non-stationary data\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}